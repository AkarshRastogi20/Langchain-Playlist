{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Create main.py file**"
      ],
      "metadata": {
        "id": "Jqmm7EPh_gAW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uRaWkGU_N-r"
      },
      "outputs": [],
      "source": [
        "## Integrate our code OpenAI API\n",
        "import os\n",
        "from constants import openai_key # this open api key has our api key itself\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]=openai_key # for setup the environment\n",
        "\n",
        "# streamlit framework\n",
        "\n",
        "st.title('Langchain Demo With OPENAI API')\n",
        "input_text=st.text_input(\"Search the topic u want\")\n",
        "\n",
        "## OPENAI LLMS\n",
        "llm=OpenAI(temperature=0.8) # temperature is ranges between 0 to 7\n",
        "# It gives us the balanced answer , as much as the temp. value will be decreasing agent loss control on balanced answer\n",
        "\n",
        "\n",
        "if input_text:\n",
        "    st.write(llm(input_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create requirement.txt file**"
      ],
      "metadata": {
        "id": "oBe-BTAtB0JH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai\n",
        "langchain\n",
        "streamlit"
      ],
      "metadata": {
        "id": "Sio74kPSB4G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create example.py file**"
      ],
      "metadata": {
        "id": "uVKu2cE4B-d5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Integrate our code OpenAI API\n",
        "import os\n",
        "from constants import openai_key\n",
        "from langchain.llms import OpenAI\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain # used while working with prompt templates\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "from langchain.chains import SequentialChain\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]=openai_key\n",
        "\n",
        "# streamlit framework\n",
        "\n",
        "st.title('Celebrity Search Results')\n",
        "input_text=st.text_input(\"Search the topic u want\")\n",
        "\n",
        "# Prompt Templates (Here we will specify our tasks)\n",
        "\n",
        "first_input_prompt=PromptTemplate(\n",
        "    input_variables=['name'],\n",
        "    template=\"Tell me about celebrity {name}\"\n",
        ")\n",
        "\n",
        "# Memory (For storing conversation)\n",
        "\n",
        "person_memory = ConversationBufferMemory(input_key='name', memory_key='chat_history')\n",
        "dob_memory = ConversationBufferMemory(input_key='person', memory_key='chat_history')\n",
        "descr_memory = ConversationBufferMemory(input_key='dob', memory_key='description_history')\n",
        "\n",
        "## OPENAI LLMS\n",
        "llm=OpenAI(temperature=0.8)\n",
        "chain=LLMChain(  # using llm chain for your prompts\n",
        "    llm=llm,prompt=first_input_prompt,verbose=True,output_key='person',memory=person_memory)\n",
        "\n",
        "# Prompt Templates (these are multiple prompts)\n",
        "\n",
        "second_input_prompt=PromptTemplate(\n",
        "    input_variables=['person'],\n",
        "    template=\"when was {person} born\"\n",
        ")\n",
        "\n",
        "chain2=LLMChain( # llmchain for second prompt\n",
        "    llm=llm,prompt=second_input_prompt,verbose=True,output_key='dob',memory=dob_memory)\n",
        "# Prompt Templates\n",
        "\n",
        "third_input_prompt=PromptTemplate(  # llmchain for third prompt\n",
        "    input_variables=['dob'],\n",
        "    template=\"Mention 5 major events happened around {dob} in the world\"\n",
        ")\n",
        "chain3=LLMChain(llm=llm,prompt=third_input_prompt,verbose=True,output_key='description',memory=descr_memory)\n",
        "\n",
        "parent_chain=SequentialChain(  # Combining the chain\n",
        "    chains=[chain,chain2,chain3],input_variables=['name'],output_variables=['person','dob','description'],verbose=True)\n",
        "\n",
        "\n",
        "\n",
        "if input_text:\n",
        "    st.write(parent_chain({'name':input_text}))\n",
        "\n",
        "    with st.expander('Person Name'):\n",
        "        st.info(person_memory.buffer)\n",
        "\n",
        "    with st.expander('Major Events'):\n",
        "        st.info(descr_memory.buffer)"
      ],
      "metadata": {
        "id": "Ubxeqq-pJdqs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}