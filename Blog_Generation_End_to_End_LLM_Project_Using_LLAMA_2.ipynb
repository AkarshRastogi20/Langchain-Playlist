{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "LLAMA 2 is open source llm model which is created by meta(facebook)"
      ],
      "metadata": {
        "id": "K5eEA5iC71pg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create requirement.txt file**"
      ],
      "metadata": {
        "id": "rn-vyBzSIFPP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3SEqZj17pGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7053f6eb-2760-4a69-a8f9-c9802bfb40ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "sentence-transformers\n",
        "uvicorn\n",
        "ctransformers\n",
        "langchain\n",
        "python-box\n",
        "streamlit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import CTransformers\n",
        "\n",
        "\n",
        "## Function to get response from Llama2 model\n",
        "\n",
        "def getLlamaresponse(input_text,no_words,blog_style):\n",
        "\n",
        "    ## Calling our Llama2 model which we have to download from huggingface\n",
        "\n",
        "    llm=CTransformers(model=\"decapoda-research/llama-2-7b-chat-hf\",model_type='llama',config={'max_new_tokens':256,'temperature':0.01}) # paste entire llama 2 downloaded path\n",
        "\n",
        "# write prompt templates\n",
        "\n",
        "    template=\"\"\"\n",
        "      Write a blog for {blog_style} job profile for a topic {input_text}\n",
        "      within {no_words} words.\n",
        "          \"\"\"\n",
        "\n",
        "    prompt=PromptTemplate(input_variables=['blog_style','input_text','no_words'],\n",
        "                          template=template)\n",
        "\n",
        "\n",
        "    # Generate the response from the llama model\n",
        "\n",
        "    response=llm(prompt.format(blog_style=blog_style,input_text=input_text,no_words=no_words))\n",
        "    return response\n",
        "    print(response)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "st.set_page_config(page_title=\"Generate Blogs\",\n",
        "                   page_icon='@',\n",
        "                   layout='centered',\n",
        "                   initial_sidebar_state='collapsed')\n",
        "\n",
        "st.header(\"Generate Blogs\")\n",
        "\n",
        "input_text=st.text_input(\"Enter the blog topic\")\n",
        "\n",
        "## creating 2 more columns\n",
        "\n",
        "col1,col2=st.columns([5,5])\n",
        "\n",
        "with col1:\n",
        "    no_words=st.text_input('No.of words')\n",
        "\n",
        "with col2:\n",
        "    blog_style=st.selectbox('Writing the blog for',('Researchers','Data scientists','Common people'),index=0)\n",
        "\n",
        "\n",
        "submit=st.button(\"Generate\")\n",
        "\n",
        "\n",
        "## final response\n",
        "if submit:\n",
        "  st.write(getLlamaresponse(input_text,no_words,blog_style))\n",
        "\n"
      ],
      "metadata": {
        "id": "qiKnC6rsQajQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}